{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW와 TF IDF 모형을 사용한 자연어 Classification 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore')               # Turn the warnings off.\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 데이터를 읽어온다.\n",
    "영화 리뷰 데이터. <br>\n",
    "- 데이터의 출처는 [여기](https://perun.pmf.uns.ac.rs/radovanovic/dmsem/cd/datasets/text/MovieReviews/Movie%20Review%20Data.htm).<br>\n",
    "- 2000개의 파일로 이루어진 데이터를 Pickle로 한 개의 파일로 저장해 놓고 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle 되어 있는 데이터를 읽어온다.\n",
    "with open('../data/data_reviews.pkl','rb') as f:\n",
    "    reviews = pickle.load(f)\n",
    "my_docs, y = reviews.data, reviews.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1000, 1000], dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 전처리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(my_docs)):\n",
    "    review = re.sub(r'\\W', ' ', str(my_docs[i]))\n",
    "    review = review.lower()\n",
    "    review = re.sub(r'^br$', ' ', review)\n",
    "    review = re.sub(r'\\s+br\\s+',' ',review)      \n",
    "    review = re.sub(r'\\s+[a-z]\\s+', ' ',review)  \n",
    "    review = re.sub(r'^b\\s+', '', review)             \n",
    "    review = re.sub(r'\\s+', ' ', review)               # 잉여 space 제거.\n",
    "    review = re.sub(\",|\\n|@|:\", \"\", review) # 쉼표, \\n, @ 제거\n",
    "    review = re.sub(r'\\([^)]*\\)', '', review) # 소괄호 제거\n",
    "    corpus.append(review)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. BOW Classification 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. BOW 행렬을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOW = CountVectorizer(max_features = 1000, stop_words = stopwords.words('english'))\n",
    "BOW.fit(corpus)                                          # BOW 모델 학습. \n",
    "X = BOW.transform(corpus).toarray()                      # \"transform\" 하므로 행렬이 생성됨.\n",
    "#X = BOW.fit_transform(corpus).toarray()                 # 한번의 스텝으로 처리.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 로지스틱 회귀 예측."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training.\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing.\n",
    "y_pred = LR.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785\n"
     ]
    }
   ],
   "source": [
    "# 정확도.\n",
    "print(np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TF IDF Classification 예측."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. TF IDF 행렬을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF = TfidfVectorizer(max_features = 1000, min_df = 2, max_df = 0.6, stop_words = stopwords.words('english'))\n",
    "TFIDF.fit(corpus)                                          # TF-IDF 모델 학습. \n",
    "X = TFIDF.transform(corpus).toarray()                      # \"transform\" 하므로 행렬이 생성됨.\n",
    "#X = TFIDF.fit_transform(corpus).toarray()                 # 한번의 스텝으로 처리.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. 로지스틱 회귀 예측."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training.\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing.\n",
    "y_pred = LR.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n"
     ]
    }
   ],
   "source": [
    "# 정확도.\n",
    "print(np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** TF IDF가 BOW 보다 다소 향상된 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Doc2Vec 예측."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arnold schwarzenegger has been an icon for act...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good films are hard to find these days ngreat ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quaid stars as man who has taken up the proffe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we could paraphrase michelle pfieffer characte...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kolya is one of the richest films ve seen in s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>under any other circumstances would not be dis...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>bruce barth mellow piano plays in the backgrou...</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>man is not man without eight taels of gold nst...</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>this is film that was inclined to like at the ...</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>any remake of an alfred hitchcock film is at b...</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     rt   num\n",
       "0     arnold schwarzenegger has been an icon for act...     0\n",
       "1     good films are hard to find these days ngreat ...     1\n",
       "2     quaid stars as man who has taken up the proffe...     2\n",
       "3     we could paraphrase michelle pfieffer characte...     3\n",
       "4     kolya is one of the richest films ve seen in s...     4\n",
       "...                                                 ...   ...\n",
       "1995  under any other circumstances would not be dis...  1995\n",
       "1996  bruce barth mellow piano plays in the backgrou...  1996\n",
       "1997  man is not man without eight taels of gold nst...  1997\n",
       "1998  this is film that was inclined to like at the ...  1998\n",
       "1999  any remake of an alfred hitchcock film is at b...  1999\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus = pd.DataFrame({'rt':corpus})\n",
    "my_corpus['num'] = pd.DataFrame(range(len(my_corpus)))\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt</th>\n",
       "      <th>num</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arnold schwarzenegger has been an icon for act...</td>\n",
       "      <td>0</td>\n",
       "      <td>[arnold, schwarzenegger, has, been, an, icon, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good films are hard to find these days ngreat ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[good, films, are, hard, to, find, these, days...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quaid stars as man who has taken up the proffe...</td>\n",
       "      <td>2</td>\n",
       "      <td>[quaid, stars, as, man, who, has, taken, up, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we could paraphrase michelle pfieffer characte...</td>\n",
       "      <td>3</td>\n",
       "      <td>[we, could, paraphrase, michelle, pfieffer, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kolya is one of the richest films ve seen in s...</td>\n",
       "      <td>4</td>\n",
       "      <td>[kolya, is, one, of, the, richest, films, ve, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>under any other circumstances would not be dis...</td>\n",
       "      <td>1995</td>\n",
       "      <td>[under, any, other, circumstances, would, not,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>bruce barth mellow piano plays in the backgrou...</td>\n",
       "      <td>1996</td>\n",
       "      <td>[bruce, barth, mellow, piano, plays, in, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>man is not man without eight taels of gold nst...</td>\n",
       "      <td>1997</td>\n",
       "      <td>[man, is, not, man, without, eight, taels, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>this is film that was inclined to like at the ...</td>\n",
       "      <td>1998</td>\n",
       "      <td>[this, is, film, that, was, inclined, to, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>any remake of an alfred hitchcock film is at b...</td>\n",
       "      <td>1999</td>\n",
       "      <td>[any, remake, of, an, alfred, hitchcock, film,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     rt   num  \\\n",
       "0     arnold schwarzenegger has been an icon for act...     0   \n",
       "1     good films are hard to find these days ngreat ...     1   \n",
       "2     quaid stars as man who has taken up the proffe...     2   \n",
       "3     we could paraphrase michelle pfieffer characte...     3   \n",
       "4     kolya is one of the richest films ve seen in s...     4   \n",
       "...                                                 ...   ...   \n",
       "1995  under any other circumstances would not be dis...  1995   \n",
       "1996  bruce barth mellow piano plays in the backgrou...  1996   \n",
       "1997  man is not man without eight taels of gold nst...  1997   \n",
       "1998  this is film that was inclined to like at the ...  1998   \n",
       "1999  any remake of an alfred hitchcock film is at b...  1999   \n",
       "\n",
       "                                                     wd  \n",
       "0     [arnold, schwarzenegger, has, been, an, icon, ...  \n",
       "1     [good, films, are, hard, to, find, these, days...  \n",
       "2     [quaid, stars, as, man, who, has, taken, up, t...  \n",
       "3     [we, could, paraphrase, michelle, pfieffer, ch...  \n",
       "4     [kolya, is, one, of, the, richest, films, ve, ...  \n",
       "...                                                 ...  \n",
       "1995  [under, any, other, circumstances, would, not,...  \n",
       "1996  [bruce, barth, mellow, piano, plays, in, the, ...  \n",
       "1997  [man, is, not, man, without, eight, taels, of,...  \n",
       "1998  [this, is, film, that, was, inclined, to, like...  \n",
       "1999  [any, remake, of, an, alfred, hitchcock, film,...  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 분절\n",
    "def nltk_tokenizer(_wd):\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(_wd.lower())\n",
    "\n",
    "my_corpus['wd'] = my_corpus['rt'].apply(nltk_tokenizer)\n",
    "my_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267473\n",
      "44556\n"
     ]
    }
   ],
   "source": [
    "tokens = [ t for d in my_corpus['wd'] for t in d]\n",
    "text = nltk.Text(tokens, name='AI_assay')\n",
    "print(len(text.tokens))\n",
    "print(len(set(text.tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sutra', 1), ('kama', 1), ('choudhury', 1), ('sarita', 1), ('gekko', 1), ('mover', 1), ('ironing', 1), ('minimalistic', 1), ('motorists', 1), ('nmorse', 1), ('inveigles', 1), ('impatience', 1), ('resignedly', 1), ('poled', 1), ('rafts', 1), ('soong', 1), ('sundry', 1), ('lobsters', 1), ('lighters', 1), ('beekeeping', 1), ('beekeeper', 1), ('npressure', 1), ('nnu', 1), ('tempest', 1), ('barth', 1), ('dateline', 1), ('munchausen', 1), ('negates', 1), ('bamboozled', 1), ('bludgeoned', 1), ('intruded', 1), ('sharpening', 1), ('indigestion', 1), ('repetitiously', 1), ('bloodstains', 1), ('sprinkler', 1), ('exclusivity', 1), ('harridans', 1), ('karnstein', 1), ('conducive', 1), ('nhes', 1), ('somersets', 1), ('nsomersets', 1), ('finchers', 1), ('relentlessness', 1), ('classifying', 1), ('losin', 1), ('toiled', 1), ('fringes', 1), ('hanksian', 1), ('ndowny', 1), ('downy', 1), ('chivalrous', 1), ('gaskell', 1), ('chivalrously', 1), ('toneless', 1), ('writerly', 1), ('bathrobes', 1), ('craggy', 1), ('vehemence', 1), ('galahad', 1), ('endowing', 1), ('beirut', 1), ('hendren', 1), ('tippi', 1), ('nloaded', 1), ('genuises', 1), ('batallion', 1), ('nbiggs', 1), ('rivaled', 1), ('suggestiveness', 1), ('replenishing', 1), ('snipping', 1), ('ncompounding', 1), ('nang', 1), ('dun', 1), ('kuo', 1), ('schamus', 1), ('ncheng', 1), ('ziyi', 1), ('yon', 1), ('wristed', 1), ('balletic', 1), ('mystique', 1), ('purer', 1), ('etymology', 1), ('viceversa', 1), ('motherfucker', 1), ('letterbox', 1), ('reminicent', 1), ('gimp', 1), ('winde', 1), ('heirloom', 1), ('mcclain', 1), ('pounder', 1), ('jurgen', 1), ('counteract', 1), ('monomaniacal', 1), ('unexplainable', 1), ('2065', 1), ('sakaguchi', 1), ('hironobu', 1), ('pimples', 1), ('squaresoft', 1), ('nabruptly', 1), ('dimensionally', 1), ('thing__about', 1), ('thing__not', 1), ('uncommercial', 1), ('woburn', 1), ('trendily', 1), ('nbrunette', 1), ('forfeited', 1), ('crowned', 1), ('subliminally', 1), ('vaus', 1), ('backpedaling', 1), ('accomplishing', 1), ('deluding', 1), ('castleton', 1), ('predestination', 1), ('canran', 1), ('xtdl', 1), ('nwww', 1), ('ndukakis', 1), ('ndunn', 1), ('prying', 1), ('videographer', 1), ('squared', 1), ('cheapo', 1), ('undid', 1), ('detonated', 1), ('orville', 1), ('robyn', 1), ('n1521', 1), ('nightie', 1), ('accidentlly', 1), ('inventory', 1), ('jandt', 1), ('genocide', 1), ('cannons', 1), ('nphoton', 1), ('nphasers', 1), ('nsonic', 1), ('milky', 1), ('fractures', 1), ('sargeant', 1), ('pledging', 1), ('naziesque', 1), ('toosie', 1), ('tithe', 1), ('rebaptizes', 1), ('njessie', 1), ('orators', 1), ('claps', 1), ('gantry', 1), ('weekened', 1), ('deflowered', 1), ('dateless', 1), ('terminating', 1), ('mccardie', 1), ('disserve', 1), ('ncheri', 1), ('npatric', 1), ('natassja', 1), ('catalysts', 1), ('ulation', 1), ('manip', 1), ('infidelitous', 1), ('nvelicorapters', 1), ('croissant', 1), ('nshame', 1), ('ngulp', 1), ('godzila', 1), ('textbooks', 1), ('playthings', 1), ('doens', 1), ('baldly', 1), ('secondhand', 1), ('niggy', 1), ('gaunt', 1), ('ntattooed', 1), ('shivery', 1), ('nperez', 1), ('retrieved', 1), ('worthiness', 1), ('exacted', 1), ('backtracks', 1), ('nstubbornly', 1), ('ntong', 1), ('precipices', 1), ('sledgehammers', 1), ('chinlund', 1), ('backus', 1), ('roster', 1), ('feasting', 1), ('alpine', 1), ('nsally', 1), ('pollsters', 1), ('trampled', 1), ('tarts', 1), ('nwags', 1), ('usda', 1), ('pekurny', 1), ('nprogram', 1), ('follies', 1), ('discouragingly', 1), ('homoeroticism', 1), ('doreen', 1), ('bruiser', 1), ('heflin', 1), ('izod', 1), ('scrape', 1), ('smokealot', 1), ('swapping', 1), ('keels', 1), ('munchie', 1), ('xxv', 1), ('foreheads', 1), ('installations', 1), ('weeds', 1), ('shrubs', 1), ('crawled', 1), ('zillion', 1), ('youull', 1), ('thereus', 1), ('itus', 1), ('preachiness', 1), ('lulled', 1), ('nurture', 1), ('fullfill', 1), ('endowment', 1), ('cuckold', 1), ('dysfuntion', 1), ('experess', 1), ('hadnut', 1), ('doesnut', 1), ('omnipresence', 1), ('bussing', 1), ('scouts', 1), ('resuscitate', 1), ('pornogrpahy', 1), ('rectified', 1), ('retrieving', 1), ('swanbeck', 1), ('reload', 1), ('interrupting', 1), ('invasive', 1), ('enamoured', 1), ('klingman', 1), ('dalliances', 1), ('npollock', 1), ('conniptions', 1), ('harmonious', 1), ('cubism', 1), ('n_pollock_', 1), ('editorializing', 1), ('imponderable', 1), ('demystification', 1), ('haters', 1), ('soundbite', 1), ('backhanded', 1), ('dissertations', 1), ('constructive', 1), ('naifeh', 1), ('emshwiller', 1), ('bower', 1), ('stater', 1), ('glorifies', 1), ('justifiably', 1), ('bateer', 1), ('ndove', 1), ('ngaerity', 1), ('emigrating', 1), ('eggert', 1), ('binary', 1), ('mccartney', 1), ('reissues', 1), ('lint', 1), ('unanswerable', 1), ('nfields', 1), ('heartland', 1), ('howerd', 1), ('uuuuuuggggggglllllllyyyyy', 1), ('correlate', 1), ('frampton', 1), ('gibbs', 1), ('nderived', 1), ('charleston', 1), ('26min', 1), ('ndom', 1), ('ntrainspotting', 1), ('tangle', 1), ('regretted', 1), ('defecated', 1), ('vomitted', 1), ('consciouness', 1), ('wetting', 1), ('daydream', 1), ('ntiptoeing', 1), ('ndistracted', 1), ('procuring', 1), ('mckidd', 1), ('bremner', 1), ('desperados', 1), ('shrills', 1), ('forebodings', 1), ('alleyways', 1), ('planners', 1), ('unabashed', 1), ('savviness', 1), ('twotg', 1), ('differnt', 1), ('nterrific', 1), ('nawwww', 1), ('pilion', 1), ('disagreeable', 1), ('nmisogynistic', 1), ('nsnide', 1), ('buzzcocks', 1), ('bafta', 1), ('nprentice', 1), ('foyle', 1), ('mackintosh', 1), ('requisitive', 1), ('nlightweight', 1), ('lobotomise', 1), ('victimisation', 1), ('hiself', 1), ('sensitivities', 1), ('confusions', 1), ('nchance', 1), ('lether', 1), ('heaed', 1), ('yeas', 1), ('barring', 1), ('bathes', 1), ('beatng', 1), ('neffeminate', 1), ('nbrit', 1), ('proxies', 1), ('digestions', 1), ('pudding', 1), ('corks', 1), ('procreation', 1), ('gutlessness', 1), ('selfless', 1), ('hudreds', 1), ('billiard', 1), ('draconian', 1), ('altruist', 1), ('inhereit', 1), ('marshmallow', 1), ('napproach', 1), ('wanda_', 1), ('_a', 1), ('_kingpin_', 1), ('nights_', 1), ('_boogie', 1), ('s_', 1), ('_porky', 1), ('aches', 1), ('nbelly', 1), ('erupted', 1), ('markie', 1), ('lichman', 1), ('troubador', 1), ('nbraces', 1), ('disaster_', 1), ('_flirting', 1), ('bites_', 1), ('_reality', 1), ('sunniness', 1), ('zippers', 1), ('npuh', 1), ('house_', 1), ('_animal', 1), ('grossfest', 1), ('warmest', 1), ('candies', 1), ('mianders', 1), ('ngayheart', 1), ('nfaring', 1), ('concur', 1), ('seventeenth', 1), ('envies', 1), ('deflecting', 1), ('nfouled', 1), ('waster', 1), ('nlooooooong', 1), ('fastball', 1), ('overachieving', 1), ('enmity', 1), ('isuro', 1), ('bruskotter', 1), ('leaguers', 1), ('sadsacks', 1), ('earmarks', 1), ('commentator', 1), ('hashbrown', 1), ('canary', 1), ('scoffs', 1), ('belted', 1), ('ngrudgingly', 1), ('retires', 1), ('nparamount', 1), ('nbraga', 1), ('nst', 1), ('zephram', 1), ('braga', 1), ('mesmerize', 1), ('krige', 1), ('cybernetic', 1), ('ngeordi', 1), ('moderator', 1), ('startrek', 1), ('followups', 1), ('raged', 1), ('cymbals', 1), ('debney', 1), ('hairball', 1), ('ntricking', 1), ('mosey', 1), ('nbenny', 1), ('atms', 1), ('romanceless', 1), ('adornments', 1), ('nmodernizing', 1), ('clemente', 1), ('cuarsn', 1), ('alfonso', 1), ('reacquaints', 1), ('behest', 1), ('nabandoned', 1), ('motorboat', 1), ('houdini', 1), ('cady', 1), ('spoonful', 1), ('magwitch', 1), ('torturously', 1), ('bails', 1), ('sitcomish', 1), ('gardenia', 1), ('remarrying', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화\n",
    "lower_cnt = int(len(set(text.tokens)) * 0.01) * -1\n",
    "print(text.vocab().most_common()[:lower_cnt:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = my_corpus[['num','wd']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[uid]) for uid, _d in doc_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# doc2vec 하이퍼 파라미터 튜닝\n",
    "max_epochs = 11\n",
    "\n",
    "model = Doc2Vec(\n",
    "    window=5,                # 문맥을 고려하는 단어의 최대 거리\n",
    "    vector_size=130,         # 벡터 크기\n",
    "    alpha=0.025,             # 모델의 학습속도\n",
    "    min_alpha=0.025,\n",
    "    workers = 8,             # 학습을 병렬화하기 위한 스레드 수\n",
    "    min_count=3,             # 최소 단어 빈도\n",
    "    dm =0,                   # dm = '0': DBOW // dm = '1': DM\n",
    "    negative = 6,            # 음수 에제의 수를 결정\n",
    "    seed = 9999)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    model.alpha -= 0.002     # 학습 속도 점차 감소시킴\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장에 대한 임베딩 추정\n",
    "vectors = [model.infer_vector(my_cor.split()) for my_cor in my_corpus['rt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training.\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing.\n",
    "y_pred = LR.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.887\n"
     ]
    }
   ],
   "source": [
    "# 정확도.\n",
    "print(np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** TF IDF와 BOW 보다 doc2vec이 10% 가까이 향상된 성능을 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
